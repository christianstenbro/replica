{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages * CLEAN NON USED MODULES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, RocCurveDisplay \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up paths\n",
    "First step is to setup files paths for the linguistics database and word2vec model made by Yang et al. 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting up relative file paths\n",
    "\n",
    "# get current working directory\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "\n",
    "# relative file path for linguistics database\n",
    "data_file_rel_path = \"../data/paper_data_streamlined.csv\"\n",
    "\n",
    "# make absolute path for linguistics database by combinig current working directory and relative path\n",
    "data_path = (current_dir / data_file_rel_path).resolve()\n",
    "\n",
    "\n",
    "# relative file path for word to vec model\n",
    "model_rel_path = \"../data/mag_200d_psy_eco_word2vec\"\n",
    "\n",
    "# make absolute path for word2vec model by combinig current working directory and relative path\n",
    "model_path  = (current_dir / model_rel_path).resolve()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in linguistics database and word2vec model\n",
    "The linguistics database is loaded in as the object \"data\", and the word2vec model is loaded in as \"model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in linguistics database\n",
    "data = pd.read_csv(data_path, sep=',', names=['ID', 'bib', 'abstract', 'rep_score'])\n",
    "\n",
    "# Removes some rows from the dataframe with NAs (remember that python starts counting from 0 . . .):\n",
    "### CONSIDER MOVING TO R *\n",
    "data.drop([0, 43, 92], axis = 0, inplace = True)\n",
    "\n",
    "\n",
    "# Importing the word2vec model as a dataframe:\n",
    "model = pd.read_csv(model_path, sep=' ', skiprows = 1, header=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate TF-IDF vectors for each paper\n",
    "A TF-IDF vector is calculated for each documents in the collection. Each document is a abstract in the linguistics database, and the collection refers to the entire database of abstracts. \n",
    "\n",
    "This is done using the tfidfVectorizer function from scikit-learn. The vectors for each paper are saved as rows in a matrix. Each entry is the TF-IDF for a term in entire collection. Since many terms are not present in all documents, the resulting TF-IDFs will often be zero. The matrix is therefore saved as a sparse matrix, which is computationally more efficient. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF-IDF vectorizer function from sci-kit learn\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Make a sparse matrix containing TF-IDFs for each abstract. \n",
    "# This is done by passing the column containg Abstracts from the linguistics database to the vectorizer function. \n",
    "# Note that thext has be to converted to unicode strings. the text needs to be converted to unicode strings (see https://stackoverflow.com/questions/39303912/tfidfvectorizer-in-scikit-learn-valueerror-np-nan-is-an-invalid-document)\n",
    "matrix = tfidf_vectorizer.fit_transform(data['abstract'].values.astype('U')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with the dictionary file from the TF-IDF\n",
    "The TF-IDF matrix comes with .vocabulary_ method which returns an dictionary. The dictionary is structured where the keys are terms, and the values are the column index of the term in TF-IDF vector. We reverse the dictionary so that keys are index-locations in tf-idf matrix and values are the corresponding term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list (techically dict_item) of tuples that are (term, index) from the vocabulary dictionary \n",
    "items = dict.items(tfidf_vectorizer.vocabulary_) \n",
    "\n",
    "# Turn this list of tuples into a data frame with two columns. One is term, the other is index.\n",
    "dict_df = pd.DataFrame(items)\n",
    "\n",
    "# Rename columns with sensible names\n",
    "dict_df.columns = ['keys', 'values']\n",
    "\n",
    "#Create a list with the order we want for the dataframe\n",
    "column_titles = ['values', 'keys']\n",
    "\n",
    "# Flip the order of columns in df using the list\n",
    "dict_df = dict_df.reindex(columns = column_titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              the\n",
       "1               of\n",
       "2              and\n",
       "3               in\n",
       "4               to\n",
       "            ...   \n",
       "275556    workover\n",
       "275557    condotel\n",
       "275558      kuntey\n",
       "275559       houga\n",
       "275560      gp-stn\n",
       "Name: 0, Length: 275561, dtype: object"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_df.shape\n",
    "model.shape\n",
    "\n",
    "model.iloc[:,0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the TF-IDF and word2vec model commensurable\n",
    "In order to be able to multiply the TF-IDF vecotors with the word2vec model, we have make sure that they contain excactly the same terms. This is not the case now, which also results in the two matricises not being commnesurable. As a consequence, we have to remove all terms from the word2vec model that are not present in the TF-IDF. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove terms from word2vec model that are not present in the TF-IDF matrix\n",
    "The following code iterates through the reversed dataframe of TF-IDF (term, index) pairs. For each term, the loop checks if the same term is present in the word2vec model. This results in a dataframe, where each row is a word vector for a term in the TF-IDF. If the match isn't found, the term is stored in another dataframe of missing matches. The rows of this filtered word2vec-model is also ordered as the columns of TF-IDF matrix.  * Are the matricies still aligned when TF-IDF is changed later? * Im unsure of the how the code works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out a for loop to extract terms from the model: \n",
    "\n",
    "# data 1 = dict_df \n",
    "# data 2 = model\n",
    "\n",
    "# First we create an empty data frame to store the matched rows from the model\n",
    "matched_df = pd.DataFrame(columns = model.columns)\n",
    "\n",
    "# Then we create an empty dataframe to store the missing matches\n",
    "missing_matches = pd.DataFrame(columns=['Term', 'Value'])\n",
    "\n",
    "# We iterate through each term in the model:\n",
    "for i, term in enumerate(dict_df['keys']):\n",
    "    # Checking if the terms exists in the dictionary\n",
    "    if term in model.iloc[:,0].values:\n",
    "        # If there is a match, we extract the entire row from the model and add it to the matched_df\n",
    "        row = model.loc[model.iloc[:,0] == term] # is this extracting the row?\n",
    "        row.index = [i]\n",
    "        matched_df = pd.concat([matched_df, row]) # how does this line work?\n",
    "    else:\n",
    "        # If no match is found, add the term to missing_matches\n",
    "        missing_matches.loc[len(missing_matches)] = [term, dict_df.iloc[i, 0]]\n",
    "\n",
    "matched_df = matched_df.sort_index()    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove missing words from TF-IDF matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the TF-IDF matrix from sparse to pandas dataframe\n",
    "tf_idf_df = pd.DataFrame.sparse.from_spmatrix(matrix)\n",
    "\n",
    "# make a list of the indicies of all columns in the TF-IDF matrix that didn't have a match in word2vec model.\n",
    "missing_matches_list = list(missing_matches['Value'])\n",
    "\n",
    "# Save only columns in TF-IDF matrix that are not present in the list of missing matches \n",
    "tf_idf_matched_df = tf_idf_df[tf_idf_df.columns[~tf_idf_df.columns.isin(missing_matches_list)]]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing and aligning indecies in TF-IDF matrix and in Word2vec matrix\n",
    "Since we have removed rows and columns from the matricies, the indicies of the rows and columns doesn't line up with their actual placement. For example row 2000 in the word2vec model doesn't have id 1999. This is fixed in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the TF-IDF dataframe:  (95, 2501)\n",
      "Type of TF-IDF dataframe:  <class 'pandas.core.frame.DataFrame'>\n",
      " \n",
      "Dimensions of the word2vec dataframe:  (2501, 200)\n",
      "Type of word2vec dataframe:  <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#Select all but the first column in the matched df and save it to new matrix. This is done since the first row of the matrix has the actual terms.\n",
    "word2vec_rindx = matched_df.iloc[:, 1:] \n",
    "\n",
    "#Fix row index so that no values are skipped\n",
    "word2vec_rindx = word2vec_rindx.reset_index(drop = True) # Reindexing\n",
    "\n",
    "#Fix column index so that each value corresponds to a dimension. Dimensions are 0-199\n",
    "word2vec_rindx.columns = range(len(word2vec_rindx.columns)) \n",
    "\n",
    "# Doing the same for the tf_idf.matched dataframe: *?\n",
    "tf_idf_rindx = tf_idf_matched_df.reset_index(drop = True) # Reindexing\n",
    "\n",
    "tf_idf_rindx.reset_index(drop = True, inplace = True) # and changing the index range to 0-199\n",
    "\n",
    "# Checking type and dimensions of our two dataframes:\n",
    "\n",
    "print(\"Dimensions of the TF-IDF dataframe: \", tf_idf_rindx.shape)\n",
    "print(\"Type of TF-IDF dataframe: \", type(tf_idf_rindx))\n",
    "print(\" \")\n",
    "print(\"Dimensions of the word2vec dataframe: \", word2vec_rindx.shape)\n",
    "print(\"Type of word2vec dataframe: \", type(word2vec_rindx))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiplying the matrices: TF-IDF x Word2Vec \n",
    "We now multiply the TF-IDF matrix and the word2vec matrix. The TF-IDF is a (95,2501) matrix and the word2vec is a (2501, 200) matrix. This makes them comensurable or \"multiplyable\". This results in a (95,200) matrix. This resulting matrix is corresponds to a vector for each paper that has reweighted the word vectors with the papers TF-IDF values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn the both matricies into a numpy arrays in order to do numpy operations\n",
    "tf_idf_rindx_np = tf_idf_rindx.to_numpy() \n",
    "word2vec_rindx_np = word2vec_rindx.to_numpy()\n",
    "\n",
    "#Multiply the the two \n",
    "tf_idf_w2v_product = np.matmul(tf_idf_rindx_np, word2vec_rindx_np)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append ground truths to each paper's vector\n",
    "We code each paper's 200d vector with 1 if it is judged replicated or partially replicated. 0 if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the column of yes/partial/no replication encodings from the linguistics database. Outputs a pandas Series\n",
    "rep_column = (data['rep_score'])\n",
    "\n",
    "#Turn it into a numpy array\n",
    "rep_column = rep_column.to_numpy()\n",
    "\n",
    "# Loop through each entry and turn yes/partial into 1 and 0 if not. \n",
    "for i, val in enumerate(rep_column):\n",
    "    if val == \"yes\" or i == \"partial\":\n",
    "        rep_column[i] = 1\n",
    "    else:\n",
    "        rep_column[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 201)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append outcomes to tf-idf * w2v product matrix:\n",
    "\n",
    "tf_idf_w2v_encoded = np.c_[ tf_idf_w2v_product, rep_column ]\n",
    "\n",
    "tf_idf_w2v_encoded.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final matrices:\n",
    "\n",
    "- tf_w2v_encoded\n",
    "\n",
    "- tf_idf_w2v_encoded "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====== Training model ======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove last column and add to y vector\n",
    "# if len(tf_idf_w2v_encoded.columns) == 201:\n",
    "#     y = tf_idf_w2v_encoded.pop(200)\n",
    "\n",
    "# #Predictors\n",
    "# X = tf_idf_w2v_encoded\n",
    "\n",
    "# # Train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "# # Stratify = y means that the training set will be balanced between the classes\n",
    "# # Random_state = 42 is a random seed, to ensure that the otherwise randomized train-test-set split is the same each time we run the kernel\n",
    "\n",
    "# #Fit Randomforest\n",
    "# clf = RandomForestClassifier(max_depth=3, random_state=0)\n",
    "# #clf.fit(X_train, y_train.astype('int'))\n",
    "\n",
    "# #cross validation with grid search. Tries a bunch of hyperparameters, whatever they are, and picks the best performing model\n",
    "# space = dict()\n",
    "# space['n_estimators'] = [10,50,100]\n",
    "# space['max_features'] = [2,4,6]\n",
    "# search = GridSearchCV(clf, space, cv = KFold(), refit=True)\n",
    "# result = search.fit(X_train, y_train.astype('int'))\n",
    "# best_model = result.best_estimator_\n",
    "\n",
    "# print(best_model.get_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  1\n",
      "0  1    59\n",
      "1  0    36\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into features and target\n",
    "y = tf_idf_w2v_encoded.iloc[:, 200] # Target\n",
    "y = pd.get_dummies(y)\n",
    "X = tf_idf_w2v_encoded.iloc[:, :200] # Features\n",
    "\n",
    "# Check frequencies for the different outcomes in the dataset\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data-set into a train and test set\n",
    "\n",
    "\n",
    "\n",
    "# Strangely, just increasing the size of the test set increases accuracy. Clearly, something super random is going on.\n",
    "\n",
    "# Stratify = y means that the training set will be balanced between the classes\n",
    "# Random_state = 42 is a random seed, to ensure that the otherwise randomized train-test-set split is the same each time we run the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the model print its parameters\n",
    "sklearn.set_config(print_changed_only=False)\n",
    "\n",
    "# Instantiating the Random Forest Classifier\n",
    "forest = RandomForestClassifier(max_depth=3)\n",
    "\n",
    "#empty list of observed accuraices\n",
    "accuracies = []\n",
    "\n",
    "#make a new train-test split and test accuracy. Becomes expensive pretty fast. 100 iterations are 9secs on M1 Macbook Air\n",
    "for i in range(30):\n",
    "\n",
    "    # Make train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "    # train model for each iteration\n",
    "    forest.fit(X_train, y_train)\n",
    "\n",
    "    # generate predictions\n",
    "    y_pred_test = forest.predict(X_test)\n",
    "\n",
    "    # score predicitons\n",
    "    accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "    # save accurcy score to list\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "#turn list of accuracies into dataframe\n",
    "accuracies_df = pd.DataFrame(accuracies)\n",
    "#rename column\n",
    "accuracies_df.columns = ['accuracy']\n",
    "#save to .csv for visualization in r\n",
    "accuracies_df.to_csv(\"../data/accuracies.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the accuracy is not very impressive, it is not the only measure of how well the model actually performs. According to [Kreiger](https://medium.com/analytics-vidhya/evaluating-a-random-forest-model-9d165595ad56), we can use a confusion matrix to get more information about our model – how well does it classify the different classes (in our case 2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0]\n",
      "[0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Converting the y_pred_test to dataframe\n",
    "y_pred_test = pd.DataFrame(y_pred_test)\n",
    "\n",
    "# Checking the values out as binary vectors\n",
    "print(y_test.values.argmax(axis=1))\n",
    "print(y_pred_test.values.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  9],\n",
       "       [ 3, 18]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making confusion matrix\n",
    "confusion_matrix(y_test.values.argmax(axis=1), y_pred_test.values.argmax(axis=1))\n",
    "# Notice how the inputs are converted to binary strings using the argmax along axis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.18      0.25        11\n",
      "           1       0.67      0.86      0.75        21\n",
      "\n",
      "   micro avg       0.62      0.62      0.62        32\n",
      "   macro avg       0.53      0.52      0.50        32\n",
      "weighted avg       0.57      0.62      0.58        32\n",
      " samples avg       0.62      0.62      0.62        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the classification report\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be awesome to include the tf vectors as well into the feature set . . .\n",
    "But how is this done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33ae8dfb1e94d52557fef94390af724d75076907dd50b406f79bb7d26936956c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
