{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages * CLEAN NON USED MODULES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, RocCurveDisplay \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up paths\n",
    "First step is to setup files paths for the linguistics database and word2vec model made by Yang et al. 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting up relative file paths\n",
    "\n",
    "# get current working directory\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "\n",
    "# relative file path for linguistics database\n",
    "data_file_rel_path = \"../data/paper_data_streamlined.csv\"\n",
    "\n",
    "# make absolute path for linguistics database by combinig current working directory and relative path\n",
    "data_path = (current_dir / data_file_rel_path).resolve()\n",
    "\n",
    "\n",
    "# relative file path for word to vec model\n",
    "model_rel_path = \"../data/mag_200d_psy_eco_word2vec\"\n",
    "\n",
    "# make absolute path for word2vec model by combinig current working directory and relative path\n",
    "model_path  = (current_dir / model_rel_path).resolve()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in linguistics database and word2vec model\n",
    "The linguistics database is loaded in as the object \"data\", and the word2vec model is loaded in as \"model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in linguistics database\n",
    "data = pd.read_csv(data_path, sep=',', names=['ID', 'bib', 'abstract', 'rep_score'])\n",
    "\n",
    "# Removes some rows from the dataframe with NAs (remember that python starts counting from 0 . . .):\n",
    "### CONSIDER MOVING TO R *\n",
    "data.drop([0, 43, 92], axis = 0, inplace = True)\n",
    "\n",
    "\n",
    "# Importing the word2vec model as a dataframe:\n",
    "model = pd.read_csv(model_path, sep=' ', skiprows = 1, header=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate TF-IDF vectors for each paper\n",
    "A TF-IDF vector is calculated for each documents in the collection. Each document is a abstract in the linguistics database, and the collection refers to the entire database of abstracts. \n",
    "\n",
    "This is done using the tfidfVectorizer function from scikit-learn. The vectors for each paper are saved as rows in a matrix. Each entry is the TF-IDF for a term in entire collection. Since many terms are not present in all documents, the resulting TF-IDFs will often be zero. The matrix is therefore saved as a sparse matrix, which is computationally more efficient. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF-IDF vectorizer function from sci-kit learn\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Make a sparse matrix containing TF-IDFs for each abstract. \n",
    "# This is done by passing the column containg Abstracts from the linguistics database to the vectorizer function. \n",
    "# Note that thext has be to converted to unicode strings. the text needs to be converted to unicode strings (see https://stackoverflow.com/questions/39303912/tfidfvectorizer-in-scikit-learn-valueerror-np-nan-is-an-invalid-document)\n",
    "matrix = tfidf_vectorizer.fit_transform(data['abstract'].values.astype('U')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with the dictionary file from the TF-IDF\n",
    "The TF-IDF matrix comes with .vocabulary_ method which returns an dictionary. The dictionary is structured where the keys are terms, and the values are the column index of the term in TF-IDF vector. We reverse the dictionary so that keys are index-locations in tf-idf matrix and values are the corresponding term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list (techically dict_item) of tuples that are (term, index) from the vocabulary dictionary \n",
    "items = dict.items(tfidf_vectorizer.vocabulary_) \n",
    "\n",
    "# Turn this list of tuples into a data frame with two columns. One is term, the other is index.\n",
    "dict_df = pd.DataFrame(items)\n",
    "\n",
    "# Rename columns with sensible names\n",
    "dict_df.columns = ['keys', 'values']\n",
    "\n",
    "#Create a list with the order we want for the dataframe\n",
    "column_titles = ['values', 'keys']\n",
    "\n",
    "# Flip the order of columns in df using the list\n",
    "dict_df = dict_df.reindex(columns = column_titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              the\n",
       "1               of\n",
       "2              and\n",
       "3               in\n",
       "4               to\n",
       "            ...   \n",
       "275556    workover\n",
       "275557    condotel\n",
       "275558      kuntey\n",
       "275559       houga\n",
       "275560      gp-stn\n",
       "Name: 0, Length: 275561, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_df.shape\n",
    "model.shape\n",
    "\n",
    "model.iloc[:,0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the TF-IDF and word2vec model commensurable\n",
    "In order to be able to multiply the TF-IDF vecotors with the word2vec model, we have make sure that they contain excactly the same terms. This is not the case now, which also results in the two matricises not being commnesurable. As a consequence, we have to remove all terms from the word2vec model that are not present in the TF-IDF. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove terms from word2vec model that are not present in the TF-IDF matrix\n",
    "The following code iterates through the reversed dataframe of TF-IDF (term, index) pairs. For each term, the loop checks if the same term is present in the word2vec model. This results in a dataframe, where each row is a word vector for a term in the TF-IDF. If the match isn't found, the term is stored in another dataframe of missing matches. The rows of this filtered word2vec-model is also ordered as the columns of TF-IDF matrix.  * Are the matricies still aligned when TF-IDF is changed later? * Im unsure of the how the code works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out a for loop to extract terms from the model: \n",
    "\n",
    "# data 1 = dict_df \n",
    "# data 2 = model\n",
    "\n",
    "# First we create an empty data frame to store the matched rows from the model\n",
    "matched_df = pd.DataFrame(columns = model.columns)\n",
    "\n",
    "# Then we create an empty dataframe to store the missing matches\n",
    "missing_matches = pd.DataFrame(columns=['Term', 'Value'])\n",
    "\n",
    "# We iterate through each term in the model:\n",
    "for i, term in enumerate(dict_df['keys']):\n",
    "    # Checking if the terms exists in the dictionary\n",
    "    if term in model.iloc[:,0].values:\n",
    "        # If there is a match, we extract the entire row from the model and add it to the matched_df\n",
    "        row = model.loc[model.iloc[:,0] == term] # is this extracting the row?\n",
    "        row.index = [i]\n",
    "        matched_df = pd.concat([matched_df, row]) # how does this line work?\n",
    "    else:\n",
    "        # If no match is found, add the term to missing_matches\n",
    "        missing_matches.loc[len(missing_matches)] = [term, dict_df.iloc[i, 0]]\n",
    "\n",
    "matched_df = matched_df.sort_index()    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove missing words from TF-IDF matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the TF-IDF matrix from sparse to pandas dataframe\n",
    "tf_idf_df = pd.DataFrame.sparse.from_spmatrix(matrix)\n",
    "\n",
    "# make a list of the indicies of all columns in the TF-IDF matrix that didn't have a match in word2vec model.\n",
    "missing_matches_list = list(missing_matches['Value'])\n",
    "\n",
    "# Save only columns in TF-IDF matrix that are not present in the list of missing matches \n",
    "tf_idf_matched_df = tf_idf_df[tf_idf_df.columns[~tf_idf_df.columns.isin(missing_matches_list)]]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repairing and aligning indecies in both the TF-IDF matrix and in the Word2vec matrix\n",
    "Since we have removed rows and columns from the matricies, the indicies of the rows and columns doesn't line up with their actual placement. For example row 2000 in the word2vec model doesn't have id 1999. This is fixed in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the TF-IDF dataframe:  (95, 2501)\n",
      "Type of TF-IDF dataframe:  <class 'pandas.core.frame.DataFrame'>\n",
      " \n",
      "Dimensions of the word2vec dataframe:  (2501, 200)\n",
      "Type of word2vec dataframe:  <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#Select all but the first column in the matched df and save it to new matrix. This is done since the first row of the matrix has the actual terms.\n",
    "word2vec_rindx = matched_df.iloc[:, 1:] \n",
    "\n",
    "#Fix row index so that no values are skipped\n",
    "word2vec_rindx = word2vec_rindx.reset_index(drop = True) # Reindexing\n",
    "\n",
    "#Fix column index so that each value corresponds to a dimension. Dimensions are 0-199\n",
    "word2vec_rindx.columns = range(len(word2vec_rindx.columns)) \n",
    "\n",
    "# Doing the same for the tf_idf.matched dataframe: *?\n",
    "tf_idf_rindx = tf_idf_matched_df.reset_index(drop = True) # Reindexing\n",
    "\n",
    "tf_idf_rindx.reset_index(drop = True, inplace = True) # and changing the index range to 0-199\n",
    "\n",
    "# Checking type and dimensions of our two dataframes:\n",
    "\n",
    "print(\"Dimensions of the TF-IDF dataframe: \", tf_idf_rindx.shape)\n",
    "print(\"Type of TF-IDF dataframe: \", type(tf_idf_rindx))\n",
    "print(\" \")\n",
    "print(\"Dimensions of the word2vec dataframe: \", word2vec_rindx.shape)\n",
    "print(\"Type of word2vec dataframe: \", type(word2vec_rindx))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiplying the matrices: TF-IDF x Word2Vec \n",
    "We now multiply the TF-IDF matrix and the word2vec matrix. The TF-IDF is a (95,2501) matrix and the word2vec is a (2501, 200) matrix. This makes them comensurable or \"multiplyable\". This results in a (95,200) matrix. This resulting matrix is corresponds to a vector for each paper that has reweighted the word vectors with the papers TF-IDF values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn the both matricies into a numpy arrays in order to do numpy operations\n",
    "tf_idf_rindx_np = tf_idf_rindx.to_numpy() \n",
    "word2vec_rindx_np = word2vec_rindx.to_numpy()\n",
    "\n",
    "#Multiply the the two \n",
    "tf_idf_w2v_product = np.matmul(tf_idf_rindx_np, word2vec_rindx_np)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append ground truths to each paper's vector\n",
    "We code each paper's 200d vector with 1 if it is judged replicated or partially replicated. 0 if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the column of yes/partial/no replication encodings from the linguistics database. Outputs a pandas Series\n",
    "rep_column = (data['rep_score'])\n",
    "\n",
    "#Turn it into a numpy array\n",
    "rep_column = rep_column.to_numpy()\n",
    "\n",
    "# Loop through each entry and turn yes/partial into 1 and 0 if not. \n",
    "for i, val in enumerate(rep_column):\n",
    "    if val == \"yes\" or i == \"partial\":\n",
    "        rep_column[i] = 1\n",
    "    else:\n",
    "        rep_column[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append outcomes to tf-idf * w2v product matrix:\n",
    "tf_idf_w2v_encoded = np.c_[ tf_idf_w2v_product, rep_column ]\n",
    "\n",
    "#Turn into df\n",
    "tf_idf_w2v_encoded = pd.DataFrame(tf_idf_w2v_encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training one random forest classifier\n",
    "Data is split into features and targets and this is used to train a randomforest classifier using scikit-learn. 33% of the data is used as a test and both the classifier and a dummy model, that always predicts succesfull replication is tested on the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Dummy model =  0.65625\n",
      "Accuracy Score of Actual model =  0.625\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into target vector of targets (y) and matrix of features (X)\n",
    "y = tf_idf_w2v_encoded.iloc[:, 200] # Target\n",
    "y = pd.get_dummies(y) #*What does this do?\n",
    "X = tf_idf_w2v_encoded.iloc[:, :200] # Features\n",
    "\n",
    "# Splitting the data-set into a two feature matricies. One for training and one for testing. The same is done for a vector of targets/labels.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=21) \n",
    "\n",
    "# Instantiating the Random Forest Classifier. Max depth is set to 3 since this is done by Yang et al 2020\n",
    "forest = RandomForestClassifier(max_depth=3)\n",
    "\n",
    "# And training the model via .fit()\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "# Making a baseline dummy model predicting only successful replications and comparing it with the trained model:\n",
    "\n",
    "# Make a matrix of n-rows (corresponding to the length of y_test) with [0 1] in each row.\n",
    "dummy_model_0 = np.full((len(y_test),1), 0)\n",
    "dummy_model_1 = np.full((len(y_test), 1), 1)\n",
    "dummy_model = np.hstack((dummy_model_0, dummy_model_1))\n",
    "\n",
    "# Converting to panda dataframe to match the format of the y_test results\n",
    "dummy_model_pd = pd.DataFrame(dummy_model)\n",
    "\n",
    "# Printing the accuracy scores of the dummy model and the actual model: \n",
    "print(\"Accuracy Score of Dummy model = \", accuracy_score(y_test, dummy_model_pd)) \n",
    "print(\"Accuracy Score of Actual model = \", accuracy_score(y_test, y_pred_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training n classifiers and comparing each classifier's accuracy with the dummy classifier\n",
    "We make n random test train splits, where the 33% of the data is used for testing. For each new split a random forest classifer is trained and both the randomforest model and the dummy models accuracy is tested on the test set. This is done to get an empirical distribution of accuracy scores. The accuracy scores for each iterations are exported to a .csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean trained model accuracy:  0.615625\n",
      "Mean dummy accuracy:  0.6359375\n"
     ]
    }
   ],
   "source": [
    "#empty list of observed accuracies\n",
    "accuracies = []\n",
    "dummy_accuracy_list = []\n",
    "\n",
    "#iterations ### EXPSENSIVE ### 100 iterations is approx 9 sec on M1 Macbook Air\n",
    "n = 20\n",
    "\n",
    "#make a new train-test split and test accuracy\n",
    "for i in range(n):\n",
    "    # Make train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "    # train model for each iteration\n",
    "    forest.fit(X_train, y_train)\n",
    "    # generate predictions\n",
    "    y_pred_test = forest.predict(X_test)\n",
    "    # score predicitons\n",
    "    accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    # save accurcy score to list\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "    # save dummy list accuracies\n",
    "    dummy_accuracy_list.append(accuracy_score(y_test, dummy_model_pd))\n",
    "\n",
    "#turn list of accuracies into dataframe\n",
    "accuracies_df = pd.DataFrame(list(zip(accuracies, dummy_accuracy_list)), index=None)\n",
    "accuracies_df.columns = ['accuracy', 'dummy accuracy']\n",
    "\n",
    "#save to .csv for visualization in r\n",
    "accuracies_df.to_csv(\"../data/accuracies.csv\")\n",
    "\n",
    "#print mean scores for the curious\n",
    "print(\"Mean trained model accuracy: \",np.mean(accuracies))\n",
    "print(\"Mean dummy accuracy: \", np.mean(dummy_accuracy_list))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33ae8dfb1e94d52557fef94390af724d75076907dd50b406f79bb7d26936956c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
