{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>bib</th>\n",
       "      <th>abstract</th>\n",
       "      <th>rep_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>original_paper_ID</td>\n",
       "      <td>original_paper_bib</td>\n",
       "      <td>abstract</td>\n",
       "      <td>replication_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O1</td>\n",
       "      <td>Vitevitch, M. S. and Stamer, M. K. 2006. The c...</td>\n",
       "      <td>In previous studies in English examining the i...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O2</td>\n",
       "      <td>Cutler, A. , Mehler, J. , Noh, D. , and Seguí,...</td>\n",
       "      <td>Infants acquire whatever language is spoken in...</td>\n",
       "      <td>partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O2</td>\n",
       "      <td>Seguí, J. , 1986. The syllable's differing rol...</td>\n",
       "      <td>Speech segmentation procedures may differ in s...</td>\n",
       "      <td>partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O3</td>\n",
       "      <td>Braun , B. , &amp; Tagliapietra , L. 2009 The role...</td>\n",
       "      <td>Sentences with a contrastive intonation contou...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID                                                bib   \n",
       "0  original_paper_ID                                 original_paper_bib  \\\n",
       "1                 O1  Vitevitch, M. S. and Stamer, M. K. 2006. The c...   \n",
       "2                 O2  Cutler, A. , Mehler, J. , Noh, D. , and Seguí,...   \n",
       "3                 O2  Seguí, J. , 1986. The syllable's differing rol...   \n",
       "4                 O3  Braun , B. , & Tagliapietra , L. 2009 The role...   \n",
       "\n",
       "                                            abstract          rep_score  \n",
       "0                                           abstract  replication_score  \n",
       "1  In previous studies in English examining the i...                yes  \n",
       "2  Infants acquire whatever language is spoken in...            partial  \n",
       "3  Speech segmentation procedures may differ in s...            partial  \n",
       "4  Sentences with a contrastive intonation contou...                yes  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data and converting to panda dataframe (and changing/defining the names)\n",
    "\n",
    "data_file = '/Users/christianstenbro/Programming/ripley_project/Data_files/original_paper_data_frame_cleaned_v1.csv'\n",
    "\n",
    "data = pd.read_csv(data_file, sep=',', names=['ID', 'bib', 'abstract', 'rep_score'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 2595)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tfidf vectorizing the text and saving in a (sparse?) matrix\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "matrix = tfidf.fit_transform(data['abstract'].values.astype('U')) # the text needs to be converted to unicode strings (see https://stackoverflow.com/questions/39303912/tfidfvectorizer-in-scikit-learn-valueerror-np-nan-is-an-invalid-document)\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the vocabulary\n",
    "tfidf.vocabulary_ # is the number the overall corpus frequency associated with each word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 5\n",
    "col = tfidf.vocabulary_['syllabifying'] # how is the tfidf matrix structured?\n",
    "\n",
    "print('Abstract: \"%s\"' % data.loc[row, 'ID'])\n",
    "print('TF-IDF score: %f' % matrix[row, col])\n",
    "\n",
    "print(matrix)\n",
    "matrix.shape\n",
    "\n",
    "print(\"Column number: \", col)\n",
    "\n",
    "# So clearly, the data is there now . . . we just need to extract the right data in the matrix to do the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 2595)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ====== pt. 2: importing the word2vec model and working with the dictionary file from the tf-idf =======\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the word2vec model as dataframe:\n",
    "\n",
    "model = '/Users/christianstenbro/AU/Applied_Cognitive_Science/Rep_rep_project/prediction_models/data and code/mag_200d_psy_eco_word2vec'\n",
    "\n",
    "model = pd.read_csv(model, sep=' ', skiprows = 1, header=None)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.iloc[:,0]) # this is the column of works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining dictionary as a data frame\n",
    "\n",
    "# First, use dict.items() to get a group of the key-value pairs in the dictionary:\n",
    "\n",
    "items = dict.items(tfidf.vocabulary_)\n",
    "\n",
    "# Then, having this group as an object, use list(obj) to convert it to a list:\n",
    "\n",
    "items = list(items)\n",
    "\n",
    "# Finally, using this list as data, call numpy.array(data) to convert it to an array. But actually I want it to be a core pd data frame:\n",
    "\n",
    "dict_df = pd.DataFrame(items)\n",
    "\n",
    "print(dict_df)\n",
    "\n",
    "print(type(dict_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns:\n",
    "\n",
    "dict_df.columns = ['keys', 'values']\n",
    "\n",
    "print(dict_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# . . . and flipping the order:\n",
    "\n",
    "column_titles = ['values', 'keys']\n",
    "dict_df = dict_df.reindex(columns = column_titles)\n",
    "print(dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can index from the dataframe:\n",
    "\n",
    "dict_df['keys'] # either extracting the entire column\n",
    "dict_df['keys'][4] # or a single entry in a column\n",
    "dict_df[['keys', 'values']] # how would I extract and entire row with data from both columns though?\n",
    "\n",
    "# Link to indexing tips in Python: https://www.dataquest.io/blog/tutorial-indexing-dataframes-in-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining some attribrutes of the data:\n",
    "\n",
    "print(dict_df.columns)\n",
    "print(dict_df.index) # this is useful knowledge when wanting to construct the for loop\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task is to remove the words from the word2vec model (model) that do not appear in the dictionary (dict_df) and create a new model data-frame.\n",
    "\n",
    "- One way to do this is by making a for-loop which goes through each row of our model dataframe, checking if there is a match with any of the words in the dictionaries. \n",
    "\n",
    "- If there is a match, the **word and its associated vector of dimensional values** should be added to a new two-column data-frame. \n",
    "\n",
    "- Ultimately, this data-frame will be as long as the dictionary, which should mean that it will be commensurable with the tf-idf matrix (elaborate).\n",
    "\n",
    "> Make a plan for how to concretely design the for loop. Remember that we need to loop through the word2vec model for each word entry (row) in our dict_df!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df.shape\n",
    "model.shape\n",
    "\n",
    "model.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out a for loop to extract terms from the model: \n",
    "\n",
    "# data 1 = dict_df \n",
    "# data 2 = model\n",
    "\n",
    "# First we create an empty data frame to store the matched rows from the model\n",
    "matched_df = pd.DataFrame(columns = model.columns)\n",
    "\n",
    "# Then we create an empty dataframe to store the missing matches\n",
    "missing_matches = pd.DataFrame(columns=['Term', 'Value'])\n",
    "\n",
    "# We iterate through each term in the model:\n",
    "for i, term in enumerate(dict_df['keys']):\n",
    "    # Checking if the terms exists in the dictionary\n",
    "    if term in model.iloc[:,0].values:\n",
    "        # If there is a match, we extract the entire row from the model and add it to the matched_df\n",
    "        row = model.loc[model.iloc[:,0] == term] # is this extracting the row?\n",
    "        row.index = [i]\n",
    "        matched_df = pd.concat([matched_df, row]) # how does this line work?\n",
    "    else:\n",
    "        # If no match is found, add the term to missing_matches\n",
    "        missing_matches.loc[len(missing_matches)] = [term, dict_df.iloc[i, 0]]\n",
    "\n",
    "matched_df = matched_df.sort_index()    \n",
    "\n",
    "# Print missing matches\n",
    "print(\"Missing Matches:\")\n",
    "print(missing_matches)\n",
    "\n",
    "# Print the matched data frame\n",
    "print(\"Matched Data Frame:\")\n",
    "print(matched_df)\n",
    "\n",
    "type(matched_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ========== next step: remove missing words from TF_IDF matrix ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing matches list dimensions:  (85, 2)\n",
      "Dimensions of the new model data frame:  (2510, 201)\n",
      "Dimensions of the TF-IDF matrix:  (98, 2595)\n"
     ]
    }
   ],
   "source": [
    "# Checking some dimensions of the objects of interest:\n",
    "\n",
    "print(\"Missing matches list dimensions: \", missing_matches.shape)\n",
    "print(\"Dimensions of the new model data frame: \", matched_df.shape)\n",
    "print(\"Dimensions of the TF-IDF matrix: \", matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Term  Value\n",
      "0          syllabifying   2297\n",
      "1              grosjean   1037\n",
      "2       whethersubjects   2549\n",
      "3              thiscase   2360\n",
      "4             wordswere   2568\n",
      "..                  ...    ...\n",
      "80                 _eap     63\n",
      "81  feedbackconsistency    923\n",
      "82                  _ip     65\n",
      "83                  _ob     66\n",
      "84                 _obe     67\n",
      "\n",
      "[85 rows x 2 columns]\n",
      "values            2297\n",
      "keys      syllabifying\n",
      "Name: 164, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(missing_matches)\n",
    "# print(missing_matches['Index'])\n",
    "\n",
    "print(dict_df.iloc[164, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the matching_df as csv and sending to emil\n",
    "\n",
    "from pathlib import Path  \n",
    "filepath = Path('/Users/christianstenbro/Programming/ripley_project/Data_files/matching_df.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "matched_df.to_csv(filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflection pad:\n",
    "\n",
    "- We want to modify the for loop creating the matched_df to also output the index for the missing matches\n",
    "\n",
    "- Then, we can use this information to remove the corresponding columns from the TF-IDF\n",
    "\n",
    "**FINALLY** we are ready to somehow (?) multiply the two matrices. \n",
    "\n",
    "- We want the output to have one number for each abstract - that's all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2297\n",
       "1     1037\n",
       "2     2549\n",
       "3     2360\n",
       "4     2568\n",
       "      ... \n",
       "80      63\n",
       "81     923\n",
       "82      65\n",
       "83      66\n",
       "84      67\n",
       "Name: Value, Length: 85, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_matches['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2297, 1037, 2549, 2360, 2568, 411, 2373, 1762, 1079, 1858, 1666, 34, 53, 62, 1702, 929, 2296, 1963, 1266, 2478, 1264, 2515, 1495, 800, 1484, 1452, 1341, 2202, 235, 237, 973, 2586, 2495, 2383, 658, 904, 1087, 1625, 2491, 1588, 758, 757, 2492, 756, 161, 322, 1101, 1102, 1508, 2263, 2452, 2059, 1867, 2198, 1078, 1294, 880, 2498, 1323, 1545, 1597, 2343, 346, 2077, 2197, 1027, 344, 604, 726, 2196, 1671, 2387, 1342, 2235, 2236, 1100, 1099, 1643, 1586, 64, 63, 923, 65, 66, 67]\n"
     ]
    }
   ],
   "source": [
    "# Converting the TF-IDF matrix to panda pd\n",
    "\n",
    "tf_idf_df = pd.DataFrame.sparse.from_spmatrix(matrix)\n",
    "\n",
    "#print(tf_idf_df.shape)\n",
    "\n",
    "# Removing the columns contained in the missing_matches['Value']\n",
    "\n",
    "# Converting missing_matches['Value']\n",
    "\n",
    "type(missing_matches['Value'])\n",
    "\n",
    "missing_matches_list = list(missing_matches['Value'])\n",
    "print(missing_matches_list)\n",
    "\n",
    "# Sort tf-idf matrix and remove the columns corresponding to the values contained in the missing_matches list:\n",
    "\n",
    "tf_idf_matched_df = tf_idf_df[tf_idf_df.columns[~tf_idf_df.columns.isin(missing_matches_list)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_idf_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===== Reindexing and alligning matrices ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindexing the 'matched' word2vec dataframe\n",
    "\n",
    "word2vec_rindx = matched_df.iloc[:, 1:] # Removing the first column (with the actual words)\n",
    "word2vec_rindx = word2vec_rindx.reset_index(drop = True) # Reindexing\n",
    "word2vec_rindx.columns = range(len(word2vec_rindx.columns)) # . . . and changing the index range to 0-199\n",
    "\n",
    "print(word2vec_rindx)\n",
    "type(word2vec_rindx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the same for the tf_idf.matched dataframe:\n",
    "\n",
    "tf_idf_rindx = tf_idf_matched_df.reset_index(drop = True) # Reindexing\n",
    "tf_idf_rindx.reset_index(drop = True, inplace = True) # . . . and changing the index range to 0-199\n",
    "\n",
    "print(tf_idf_rindx)\n",
    "type(tf_idf_rindx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the TF-IDF dataframe:  (98, 2510)\n",
      "Type of TF-IDF dataframe:  <class 'pandas.core.frame.DataFrame'>\n",
      " \n",
      "Dimensions of the word2vec dataframe:  (2510, 200)\n",
      "Type of word2vec dataframe:  <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Checking type and dimensions of our two dataframes:\n",
    "\n",
    "print(\"Dimensions of the TF-IDF dataframe: \", tf_idf_rindx.shape)\n",
    "print(\"Type of TF-IDF dataframe: \", type(tf_idf_rindx))\n",
    "print(\" \")\n",
    "print(\"Dimensions of the word2vec dataframe: \", word2vec_rindx.shape)\n",
    "print(\"Type of word2vec dataframe: \", type(word2vec_rindx))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======= Multiplying the matrices: TF-IDF x Word2Vec ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# For matrices to be commensurable in a multiplication: \n",
    "# X * Y, the number of columns of X needs to correspond to the number of rows in Y\n",
    "\n",
    "# Fortunately, the number of colums in the TF-IDF dataframe (98, 2510) is the same as the\n",
    "# as the number of rows in the word2vec dataframe (2510, 200). \n",
    "\n",
    "# Mathematically, it should not be a problem to multiply the two:\n",
    "\n",
    "#product = tf_idf_rindx.dot(word2vec_rindx) \n",
    "# This does not work. We can try to use numpy?\n",
    "\n",
    "print(type(tf_idf_rindx))\n",
    "\n",
    "tf_idf_rindx_np = tf_idf_rindx.to_numpy() # Remember this parenthesis!\n",
    "\n",
    "print(type(tf_idf_rindx_np))\n",
    "\n",
    "word2vec_rindx_np = word2vec_rindx.to_numpy()\n",
    "\n",
    "print(type(word2vec_rindx_np))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product shape:  (98, 200) Product type:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Re-attempting the multiplication with numpy methods: \n",
    "\n",
    "tf_idf_w2v_product = np.matmul(tf_idf_rindx_np, word2vec_rindx_np)\n",
    "\n",
    "print(\"Product shape: \", tf_idf_w2v_product.shape, \"Product type: \", type(tf_idf_w2v_product))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====== Status update ======="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a similar matrix, using tf instead of tf_idf:\n",
    "\n",
    "    - Everything should be the same, only the input matrix should contain pure tf's instead of tf-idf's\n",
    "    \n",
    "    - We can reuse the script from pt. 1 and change one command?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-matrix dimensions:  (98, 2595) TF-matrix object type:  <class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# 1. Computing term frequencies:\n",
    "\n",
    "# data_file = '/Users/christianstenbro/Programming/ripley_project/Data_files/original_paper_data_frame_cleaned_v1.csv'\n",
    "# data = pd.read_csv(data_file, sep=',', names=['ID', 'bib', 'abstract', 'rep_score'])\n",
    "# data.head()\n",
    "\n",
    "vectorizer = CountVectorizer() # The CountVectorizer converts a text corpus (here our collection of abstracts in 'data) to a matrix of 'token counts' = term frequencies (tf).\n",
    "matrix_tf = vectorizer.fit_transform(data['abstract'].values.astype('U'))\n",
    "\n",
    "print(\"TF-matrix dimensions: \", matrix_tf.shape, \"TF-matrix object type: \", type(matrix_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the vocabularies of the tf and tfidf matrices identical? Output: True\n"
     ]
    }
   ],
   "source": [
    "vectorizer.vocabulary_ # And this matches the vocab of the tf_idf matrix:\n",
    "\n",
    "print(\"Are the vocabularies of the tf and tfidf matrices identical? Output:\", vectorizer.vocabulary_ == tfidf.vocabulary_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we can use the exact same methods to process the tf-matrix as we already used for the tf-idf matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of tf_matched_df:  (98, 2510)\n"
     ]
    }
   ],
   "source": [
    "# We already have the missing matches list from previously:\n",
    "\n",
    "missing_matches_list # The column index for the terms in the abstract matrix not present in the word2vec model. \n",
    "# These will be extracted from the tf-matrix as well to make it commensurable with the word2vec model matrix:\n",
    "\n",
    "# First, we convert the tf-matrix from a sparse matrix to a pd.DataFrame:\n",
    "\n",
    "tf_df = pd.DataFrame.sparse.from_spmatrix(matrix_tf)\n",
    "\n",
    "# Then we remove the columns correponding to the missing words in the word2vec model:\n",
    "\n",
    "tf_matched_df = tf_df[tf_df.columns[~tf_df.columns.isin(missing_matches_list)]]\n",
    "\n",
    "print(\"Dimensions of tf_matched_df: \", tf_matched_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we reindex the dataframe and convert it to a numpy array:\n",
    "\n",
    "tf_rindx = tf_matched_df.reset_index(drop = True) # Reindexing\n",
    "tf_rindx.reset_index(drop = True, inplace = True) # . . . and changing the index range to 0-199\n",
    "\n",
    "# print(tf_rindx) # One wonders if it actually contains any numbers?\n",
    "# type(tf_rindx)\n",
    "\n",
    "tf_rindx_np = tf_rindx.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the TF dataframe:  (98, 2510)\n",
      "Type of TF dataframe:  <class 'numpy.ndarray'>\n",
      " \n",
      "Dimensions of the word2vec dataframe:  (2510, 200)\n",
      "Type of word2vec dataframe:  <class 'numpy.ndarray'>\n",
      " \n",
      "Are the matrices commensurable? Output: True\n"
     ]
    }
   ],
   "source": [
    "# Again, we can check if the matrices are commensurable: \n",
    "\n",
    "print(\"Dimensions of the TF dataframe: \", tf_rindx_np.shape)\n",
    "print(\"Type of TF dataframe: \", type(tf_rindx_np))\n",
    "print(\" \")\n",
    "print(\"Dimensions of the word2vec dataframe: \", word2vec_rindx_np.shape)\n",
    "print(\"Type of word2vec dataframe: \", type(word2vec_rindx_np))\n",
    "print(\" \")\n",
    "print(\"Are the matrices commensurable? Output:\", len(tf_rindx_np[0]) == len(word2vec_rindx_np))\n",
    "\n",
    "# print(len(tf_rindx_np[0])) # Strange way of finding the column and row numbers, at least compared to R. \n",
    "# print(len(word2vec_rindx_np))\n",
    "\n",
    "# Notice that 'array[0]' is the second dimension of the array while (columns), while 'array' is simply the first dimension (rows)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====== Multiplying matrices: TF x Word2Vec ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product shape:  (98, 200) Product type:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# With everything in place, we can multiply the matrices using the np.matmul function: \n",
    "\n",
    "tf_w2v_product = np.matmul(tf_rindx_np, word2vec_rindx_np)\n",
    "\n",
    "print(\"Product shape: \", tf_w2v_product.shape, \"Product type: \", type(tf_w2v_product))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have our two matrices.\n",
    "\n",
    "But how do we actually conceptualize what these matrices consist of?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
